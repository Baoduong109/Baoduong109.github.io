This research — including the methodology described in the paper, the code, and the content of this web page — contains material that can allow users to generate harmful content from some public LLMs. Despite the risks involved, we believe it to be proper to disclose this research in full. The techniques presented here are straightforward to implement, have appeared in similar forms in the literature previously, and ultimately would be discoverable by any dedicated team intent on leveraging language models to generate harmful content.

Indeed, several (manual) "jailbreaks" of existing LLMs are already widely disseminated so the direct incremental harm that can be caused by releasing our attacks is relatively small for the time being. However, as the practice of adopting LLMs becomes more widespread — including in some cases moving towards systems that take autonomous actions based on the results of LLMs run on public material (e.g. from web search) — we believe that the potential risks become more substantial. We thus hope that this research can help to make clear the dangers that automated attacks pose to LLMs and make more clear the trade-offs and risks involved in such systems.
<pre><code>&lt;img src=x onerror=alert(document.cookie);&gt;</code></pre>
Prior to publication we disclosed the results of this study to the companies hosting the large closed-sourced LLMs that we attacked in the paper. Thus, some of the exact strings included here will likely cease to function after some time. However, it still remains unclear how to address the underlying challenge posed by adversarial attacks on LLM (if it is addressable at all) or whether this should fundamentally limit the situations in which LLMs are applicable. We hope that our work will spur future research in these directions.
$whoami
$id
$cat /etc/passwd
$cat ~/.bash_history

